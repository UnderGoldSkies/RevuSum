{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotelreviews = pd.read_csv('raw_data/Hotel_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hotelreviews.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 515738 entries, 0 to 515737\n",
      "Data columns (total 17 columns):\n",
      " #   Column                                      Non-Null Count   Dtype  \n",
      "---  ------                                      --------------   -----  \n",
      " 0   Hotel_Address                               515738 non-null  object \n",
      " 1   Additional_Number_of_Scoring                515738 non-null  int64  \n",
      " 2   Review_Date                                 515738 non-null  object \n",
      " 3   Average_Score                               515738 non-null  float64\n",
      " 4   Hotel_Name                                  515738 non-null  object \n",
      " 5   Reviewer_Nationality                        515738 non-null  object \n",
      " 6   Negative_Review                             515738 non-null  object \n",
      " 7   Review_Total_Negative_Word_Counts           515738 non-null  int64  \n",
      " 8   Total_Number_of_Reviews                     515738 non-null  int64  \n",
      " 9   Positive_Review                             515738 non-null  object \n",
      " 10  Review_Total_Positive_Word_Counts           515738 non-null  int64  \n",
      " 11  Total_Number_of_Reviews_Reviewer_Has_Given  515738 non-null  int64  \n",
      " 12  Reviewer_Score                              515738 non-null  float64\n",
      " 13  Tags                                        515738 non-null  object \n",
      " 14  days_since_review                           515738 non-null  object \n",
      " 15  lat                                         512470 non-null  float64\n",
      " 16  lng                                         512470 non-null  float64\n",
      "dtypes: float64(4), int64(5), object(8)\n",
      "memory usage: 66.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotel_Address</th>\n",
       "      <th>Additional_Number_of_Scoring</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Average_Score</th>\n",
       "      <th>Hotel_Name</th>\n",
       "      <th>Reviewer_Nationality</th>\n",
       "      <th>Negative_Review</th>\n",
       "      <th>Review_Total_Negative_Word_Counts</th>\n",
       "      <th>Total_Number_of_Reviews</th>\n",
       "      <th>Positive_Review</th>\n",
       "      <th>Review_Total_Positive_Word_Counts</th>\n",
       "      <th>Total_Number_of_Reviews_Reviewer_Has_Given</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "      <th>Tags</th>\n",
       "      <th>days_since_review</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n",
       "      <td>194</td>\n",
       "      <td>8/3/2017</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Hotel Arena</td>\n",
       "      <td>Russia</td>\n",
       "      <td>I am so angry that i made this post available...</td>\n",
       "      <td>397</td>\n",
       "      <td>1403</td>\n",
       "      <td>Only the park outside of the hotel was beauti...</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[' Leisure trip ', ' Couple ', ' Duplex Double...</td>\n",
       "      <td>0 days</td>\n",
       "      <td>52.360576</td>\n",
       "      <td>4.915968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n",
       "      <td>194</td>\n",
       "      <td>8/3/2017</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Hotel Arena</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>No Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1403</td>\n",
       "      <td>No real complaints the hotel was great great ...</td>\n",
       "      <td>105</td>\n",
       "      <td>7</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[' Leisure trip ', ' Couple ', ' Duplex Double...</td>\n",
       "      <td>0 days</td>\n",
       "      <td>52.360576</td>\n",
       "      <td>4.915968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n",
       "      <td>194</td>\n",
       "      <td>7/31/2017</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Hotel Arena</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Rooms are nice but for elderly a bit difficul...</td>\n",
       "      <td>42</td>\n",
       "      <td>1403</td>\n",
       "      <td>Location was good and staff were ok It is cut...</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>[' Leisure trip ', ' Family with young childre...</td>\n",
       "      <td>3 days</td>\n",
       "      <td>52.360576</td>\n",
       "      <td>4.915968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Hotel_Address  \\\n",
       "0   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n",
       "1   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n",
       "2   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n",
       "\n",
       "   Additional_Number_of_Scoring Review_Date  Average_Score   Hotel_Name  \\\n",
       "0                           194    8/3/2017            7.7  Hotel Arena   \n",
       "1                           194    8/3/2017            7.7  Hotel Arena   \n",
       "2                           194   7/31/2017            7.7  Hotel Arena   \n",
       "\n",
       "  Reviewer_Nationality                                    Negative_Review  \\\n",
       "0              Russia    I am so angry that i made this post available...   \n",
       "1             Ireland                                         No Negative   \n",
       "2           Australia    Rooms are nice but for elderly a bit difficul...   \n",
       "\n",
       "   Review_Total_Negative_Word_Counts  Total_Number_of_Reviews  \\\n",
       "0                                397                     1403   \n",
       "1                                  0                     1403   \n",
       "2                                 42                     1403   \n",
       "\n",
       "                                     Positive_Review  \\\n",
       "0   Only the park outside of the hotel was beauti...   \n",
       "1   No real complaints the hotel was great great ...   \n",
       "2   Location was good and staff were ok It is cut...   \n",
       "\n",
       "   Review_Total_Positive_Word_Counts  \\\n",
       "0                                 11   \n",
       "1                                105   \n",
       "2                                 21   \n",
       "\n",
       "   Total_Number_of_Reviews_Reviewer_Has_Given  Reviewer_Score  \\\n",
       "0                                           7             2.9   \n",
       "1                                           7             7.5   \n",
       "2                                           9             7.1   \n",
       "\n",
       "                                                Tags days_since_review  \\\n",
       "0  [' Leisure trip ', ' Couple ', ' Duplex Double...            0 days   \n",
       "1  [' Leisure trip ', ' Couple ', ' Duplex Double...            0 days   \n",
       "2  [' Leisure trip ', ' Family with young childre...            3 days   \n",
       "\n",
       "         lat       lng  \n",
       "0  52.360576  4.915968  \n",
       "1  52.360576  4.915968  \n",
       "2  52.360576  4.915968  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#Using preprocessing from previous assignment\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # you can also choose other languages\n",
    "\n",
    "def preprocessing_stopwords(sentence):\n",
    "    sentence = sentence.strip(\"\") #whitespace\n",
    "    sentence = sentence.lower() #lowercase\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) #remove number\n",
    "    for punctuation in string.punctuation: #remove punc\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    \n",
    "    stopwords_removed = [w for w in word_tokens if not w in stop_words] \n",
    "        # Lemmatizing the verbs\n",
    "    verb_lemmatized = [                  \n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
    "        for word in stopwords_removed]\n",
    "\n",
    "    # 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [                 \n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "        for word in verb_lemmatized]\n",
    "    \n",
    "    return noun_lemmatized\n",
    "\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    sentence = sentence.strip(\"\") #whitespace\n",
    "    sentence = sentence.lower() #lowercase\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) #remove number\n",
    "    for punctuation in string.punctuation: #remove punc\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Lemmatizing the verbs\n",
    "    verb_lemmatized = [                  \n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
    "        for word in word_tokens]\n",
    "\n",
    "    # 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [                 \n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "        for word in verb_lemmatized]\n",
    "    \n",
    "    #join_list = ' '.join(noun_lemmatized)\n",
    "    \n",
    "    return noun_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing, just use 'Hotel Arena' reviews only\n",
    "\n",
    "# Create a boolean mask for 'Hotel_Name' equal to 'Hotel Arena'\n",
    "# mask = df['Hotel_Name'] == 'Hotel Arena'\n",
    "\n",
    "# Apply the mask to filter 'Hotel Arena' reviews only from the dataframe\n",
    "# filtered_review = df[mask]\n",
    "\n",
    "filtered_review = df.iloc[:200000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 1), (200000, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying preprocessing on the both positive and negative reviews\n",
    "\n",
    "clean_negative_review = []\n",
    "cleaned_positive_review = []\n",
    "\n",
    "\n",
    "for sentence in filtered_review.Negative_Review:\n",
    "    clean_negative_review.append(preprocessing_stopwords(sentence))\n",
    "\n",
    "clean_negative_df = pd.DataFrame({'Review': clean_negative_review})    \n",
    "clean_negative_df['Review'] = clean_negative_df['Review'].astype(str)\n",
    "\n",
    "\n",
    "for sentence in filtered_review.Positive_Review:\n",
    "    cleaned_positive_review.append(preprocessing_stopwords(sentence))\n",
    "    \n",
    "clean_positive_df = pd.DataFrame({'Review': cleaned_positive_review})    \n",
    "clean_positive_df['Review'] = clean_positive_df['Review'].astype(str)\n",
    "\n",
    "clean_negative_df.shape,clean_positive_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 1), (200000, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying preprocessing on the both positive and negative reviews\n",
    "\n",
    "clean_negative_review = []\n",
    "cleaned_positive_review = []\n",
    "\n",
    "\n",
    "for sentence in filtered_review.Negative_Review:\n",
    "    clean_negative_review.append(preprocessing(sentence))\n",
    "\n",
    "clean_negative_df = pd.DataFrame({'Review': clean_negative_review})    \n",
    "clean_negative_df['Review'] = clean_negative_df['Review'].astype(str)\n",
    "\n",
    "\n",
    "for sentence in filtered_review.Positive_Review:\n",
    "    cleaned_positive_review.append(preprocessing(sentence))\n",
    "    \n",
    "clean_positive_df = pd.DataFrame({'Review': cleaned_positive_review})    \n",
    "clean_positive_df['Review'] = clean_positive_df['Review'].astype(str)\n",
    "\n",
    "clean_negative_df.shape,clean_positive_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    200000\n",
       "1    200000\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of rows in the DataFrame\n",
    "num_negative_rows = clean_negative_df.shape[0]\n",
    "num_positive_rows = clean_positive_df.shape[0]\n",
    "\n",
    "# Create a new column with all 0 and 1 values based on positive/ negative\n",
    "clean_negative_df['Label'] = [0] * num_negative_rows\n",
    "clean_positive_df['Label'] = [1] * num_positive_rows\n",
    "\n",
    "clean_negative_df.head(2),clean_positive_df.head(2)\n",
    "#Combine both dataframe together to form dataset\n",
    "combined_df = pd.concat([clean_negative_df, clean_positive_df], axis=0, join='inner')\n",
    "\n",
    "combined_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = combined_df['Review']\n",
    "y = combined_df['Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, shuffle=True,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed = embedding(word2vec_transfer, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3005, 3005, 2985)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embedding need max length of sentences so loop to find max length\n",
    "\n",
    "max_length_negative = max(len(sublist) for sublist in clean_negative_df.Review)\n",
    "max_length_positive = max(len(sublist) for sublist in clean_positive_df.Review)\n",
    "max_length = max(max_length_negative,max_length_positive)\n",
    "max_length,max_length_negative,max_length_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 11:41:37.363329: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-21 11:41:37.563555: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-21 11:41:37.601765: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:41:37.601781: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-21 11:41:37.668733: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-21 11:41:38.500730: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:41:38.500853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:41:38.500858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 157. GiB for an array with shape (280000, 3005, 50) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/jack/code/UnderGoldSkies/revusum/testing.ipynb Cell 15\u001b[0m in \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jack/code/UnderGoldSkies/revusum/testing.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jack/code/UnderGoldSkies/revusum/testing.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Pad the training and test embedded sentences\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jack/code/UnderGoldSkies/revusum/testing.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m X_train_pad \u001b[39m=\u001b[39m pad_sequences(X_train_embed, dtype\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfloat32\u001b[39;49m\u001b[39m'\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m'\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49mmax_length)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jack/code/UnderGoldSkies/revusum/testing.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m X_test_pad \u001b[39m=\u001b[39m pad_sequences(X_test_embed, dtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m'\u001b[39m, maxlen\u001b[39m=\u001b[39mmax_length)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/keras/utils/data_utils.py:1066\u001b[0m, in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m dtype \u001b[39m!=\u001b[39m \u001b[39mobject\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dtype_str:\n\u001b[1;32m   1060\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1061\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`dtype` \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m}\u001b[39;00m\u001b[39m is not compatible with `value`\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms type: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1062\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mYou should set `dtype=object` for variable length \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1063\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstrings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1064\u001b[0m     )\n\u001b[0;32m-> 1066\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mfull((num_samples, maxlen) \u001b[39m+\u001b[39;49m sample_shape, value, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m   1067\u001b[0m \u001b[39mfor\u001b[39;00m idx, s \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sequences):\n\u001b[1;32m   1068\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon/lib/python3.10/site-packages/numpy/core/numeric.py:343\u001b[0m, in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[1;32m    341\u001b[0m     fill_value \u001b[39m=\u001b[39m asarray(fill_value)\n\u001b[1;32m    342\u001b[0m     dtype \u001b[39m=\u001b[39m fill_value\u001b[39m.\u001b[39mdtype\n\u001b[0;32m--> 343\u001b[0m a \u001b[39m=\u001b[39m empty(shape, dtype, order)\n\u001b[1;32m    344\u001b[0m multiarray\u001b[39m.\u001b[39mcopyto(a, fill_value, casting\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    345\u001b[0m \u001b[39mreturn\u001b[39;00m a\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 157. GiB for an array with shape (280000, 3005, 50) and data type float32"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in train set {0: 287, 1: 280}\n",
      "Baseline accuracy:  0.48559670781893005\n"
     ]
    }
   ],
   "source": [
    "#Baseline model\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "counts = dict(zip(unique, counts))\n",
    "print('Number of labels in train set', counts)\n",
    "\n",
    "y_pred = 0 if counts[0] > counts[1] else 1\n",
    "\n",
    "print('Baseline accuracy: ', accuracy_score(y_test, [y_pred]*len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a simple RNN model\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(layers.Masking())\n",
    "    model.add(layers.LSTM(20, activation='tanh'))\n",
    "    model.add(layers.Dense(15, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 15s 883ms/step - loss: 0.6973 - accuracy: 0.4773 - val_loss: 0.7033 - val_accuracy: 0.4269\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 10s 809ms/step - loss: 0.6927 - accuracy: 0.5025 - val_loss: 0.6904 - val_accuracy: 0.5848\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 11s 818ms/step - loss: 0.6872 - accuracy: 0.5278 - val_loss: 0.6841 - val_accuracy: 0.5965\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 11s 839ms/step - loss: 0.6821 - accuracy: 0.5682 - val_loss: 0.7143 - val_accuracy: 0.4269\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 12s 894ms/step - loss: 0.6791 - accuracy: 0.5833 - val_loss: 0.6700 - val_accuracy: 0.6433\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 11s 874ms/step - loss: 0.6748 - accuracy: 0.5783 - val_loss: 0.6695 - val_accuracy: 0.6199\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 11s 851ms/step - loss: 0.6695 - accuracy: 0.6111 - val_loss: 0.6556 - val_accuracy: 0.6316\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 11s 836ms/step - loss: 0.6591 - accuracy: 0.6288 - val_loss: 0.6415 - val_accuracy: 0.6901\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 11s 834ms/step - loss: 0.6609 - accuracy: 0.6111 - val_loss: 0.6951 - val_accuracy: 0.5380\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 11s 818ms/step - loss: 0.6443 - accuracy: 0.6111 - val_loss: 0.6523 - val_accuracy: 0.6023\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6388 - accuracy: 0.6465 - val_loss: 0.6214 - val_accuracy: 0.6901\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 12s 931ms/step - loss: 0.6399 - accuracy: 0.6616 - val_loss: 0.6077 - val_accuracy: 0.7193\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6467 - accuracy: 0.6439 - val_loss: 0.6156 - val_accuracy: 0.6784\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 11s 859ms/step - loss: 0.6302 - accuracy: 0.6540 - val_loss: 0.6021 - val_accuracy: 0.7018\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 11s 858ms/step - loss: 0.6223 - accuracy: 0.6566 - val_loss: 0.6122 - val_accuracy: 0.6842\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 11s 863ms/step - loss: 0.6475 - accuracy: 0.6566 - val_loss: 0.6099 - val_accuracy: 0.6725\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 11s 882ms/step - loss: 0.6363 - accuracy: 0.6515 - val_loss: 0.7941 - val_accuracy: 0.5263\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 11s 857ms/step - loss: 0.6443 - accuracy: 0.6465 - val_loss: 0.6032 - val_accuracy: 0.6901\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 11s 856ms/step - loss: 0.6176 - accuracy: 0.6692 - val_loss: 0.5998 - val_accuracy: 0.6784\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 11s 861ms/step - loss: 0.6112 - accuracy: 0.6944 - val_loss: 0.8309 - val_accuracy: 0.5263\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 13s 993ms/step - loss: 0.6273 - accuracy: 0.6591 - val_loss: 0.6138 - val_accuracy: 0.6842\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 11s 855ms/step - loss: 0.6069 - accuracy: 0.6818 - val_loss: 0.5796 - val_accuracy: 0.6959\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 13s 1s/step - loss: 0.6095 - accuracy: 0.6692 - val_loss: 0.6269 - val_accuracy: 0.6433\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 11s 857ms/step - loss: 0.6083 - accuracy: 0.6465 - val_loss: 0.6570 - val_accuracy: 0.6140\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 11s 851ms/step - loss: 0.5811 - accuracy: 0.6919 - val_loss: 0.6316 - val_accuracy: 0.6667\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 11s 834ms/step - loss: 0.6303 - accuracy: 0.6540 - val_loss: 0.5962 - val_accuracy: 0.6842\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 11s 837ms/step - loss: 0.5915 - accuracy: 0.6540 - val_loss: 0.5953 - val_accuracy: 0.6842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2065eb3c70>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 67.490%\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying 1D-CNN (Tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#Using preprocessing from previous assignment\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # you can also choose other languages\n",
    "\n",
    "def preprocessing_stopwords_cnn(sentence):\n",
    "    sentence = sentence.strip(\"\") #whitespace\n",
    "    sentence = sentence.lower() #lowercase\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) #remove number\n",
    "    for punctuation in string.punctuation: #remove punc\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    \n",
    "    stopwords_removed = [w for w in word_tokens if not w in stop_words] \n",
    "        # Lemmatizing the verbs\n",
    "    verb_lemmatized = [                  \n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
    "        for word in stopwords_removed]\n",
    "\n",
    "    # 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [                 \n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "        for word in verb_lemmatized]\n",
    "    \n",
    "    join_list = ' '.join(noun_lemmatized)\n",
    "    \n",
    "    return join_list\n",
    "\n",
    "\n",
    "def preprocessing_cnn(sentence):\n",
    "    sentence = sentence.strip(\"\") #whitespace\n",
    "    sentence = sentence.lower() #lowercase\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) #remove number\n",
    "    for punctuation in string.punctuation: #remove punc\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "\n",
    "        # Lemmatizing the verbs\n",
    "    verb_lemmatized = [                  \n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
    "        for word in word_tokens]\n",
    "\n",
    "    # 2 - Lemmatizing the nouns\n",
    "    noun_lemmatized = [                 \n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
    "        for word in verb_lemmatized]\n",
    "    \n",
    "    join_list = ' '.join(noun_lemmatized)\n",
    "    \n",
    "    return join_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying preprocessing on the both positive and negative reviews\n",
    "\n",
    "clean_negative_review = []\n",
    "cleaned_positive_review = []\n",
    "\n",
    "\n",
    "for sentence in filtered_review.Negative_Review:\n",
    "    clean_negative_review.append(preprocessing_cnn(sentence))\n",
    "\n",
    "clean_negative_df = pd.DataFrame({'Review': clean_negative_review})    \n",
    "clean_negative_df['Review'] = clean_negative_df['Review'].astype(str)\n",
    "\n",
    "\n",
    "for sentence in filtered_review.Positive_Review:\n",
    "    cleaned_positive_review.append(preprocessing_cnn(sentence))\n",
    "    \n",
    "clean_positive_df = pd.DataFrame({'Review': cleaned_positive_review})    \n",
    "clean_positive_df['Review'] = clean_positive_df['Review'].astype(str)\n",
    "\n",
    "# Get the number of rows in the DataFrame\n",
    "num_negative_rows = clean_negative_df.shape[0]\n",
    "num_positive_rows = clean_positive_df.shape[0]\n",
    "\n",
    "# Create a new column with all 0 and 1 values based on positive/ negative\n",
    "clean_negative_df['Label'] = [0] * num_negative_rows\n",
    "clean_positive_df['Label'] = [1] * num_positive_rows\n",
    "\n",
    "clean_negative_df.head(2),clean_positive_df.head(2)\n",
    "#Combine both dataframe together to form dataset\n",
    "combined_df = pd.concat([clean_negative_df, clean_positive_df], axis=0, join='inner')\n",
    "\n",
    "X = combined_df['Review']\n",
    "y = combined_df['Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, shuffle=True,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000, 2), (200000, 2), (400000, 2))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_negative_df.shape, clean_positive_df.shape, combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "X_train_token_cnn = tk.texts_to_sequences(X_train)\n",
    "X_test_token_cnn = tk.texts_to_sequences(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_cnn = max(len(list) for list in X_train_token_cnn)\n",
    "max_test_cnn = max(len(list) for list in X_test_token_cnn)\n",
    "max_length_cnn =max(max_train_cnn,max_test_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad_cnn = pad_sequences(X_train_token_cnn, dtype='float32', padding='post',maxlen=max_length_cnn)\n",
    "X_test_pad_cnn = pad_sequences(X_test_token_cnn, dtype='float32', padding='post',maxlen=max_length_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29636"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 392, 100)          2963700   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 390, 20)           6020      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7800)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                78010     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,047,741\n",
      "Trainable params: 3,047,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-21 11:44:45.985170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-21 11:44:45.985377: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:44:45.985669: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:44:45.985735: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:44:45.985776: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:44:45.985829: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:44:45.985867: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:44:45.985902: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:44:45.986162: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-06-21 11:44:45.986174: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-06-21 11:44:45.989283: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 100\n",
    "\n",
    "# Conv1D\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1, \n",
    "    input_length=max_length_cnn, # Max_sentence_length (optional, for model summary)\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer :)\n",
    "))\n",
    "model_cnn.add(layers.Conv1D(filters=20, kernel_size=3))\n",
    "model_cnn.add(layers.Flatten())\n",
    "model_cnn.add(layers.Dense(10, activation=\"relu\"))\n",
    "model_cnn.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics='accuracy')\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6125/6125 [==============================] - 41s 7ms/step - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6932 - val_accuracy: 0.5008\n",
      "Epoch 2/100\n",
      "6125/6125 [==============================] - 38s 6ms/step - loss: 0.5246 - accuracy: 0.6440 - val_loss: 0.1510 - val_accuracy: 0.9464\n",
      "Epoch 3/100\n",
      "6125/6125 [==============================] - 37s 6ms/step - loss: 0.1482 - accuracy: 0.9481 - val_loss: 0.1519 - val_accuracy: 0.9456\n",
      "Epoch 4/100\n",
      "6125/6125 [==============================] - 38s 6ms/step - loss: 0.1422 - accuracy: 0.9509 - val_loss: 0.1468 - val_accuracy: 0.9513\n",
      "Epoch 5/100\n",
      "6125/6125 [==============================] - 39s 6ms/step - loss: 0.1406 - accuracy: 0.9515 - val_loss: 0.1429 - val_accuracy: 0.9522\n",
      "Epoch 6/100\n",
      "6125/6125 [==============================] - 38s 6ms/step - loss: 0.1381 - accuracy: 0.9522 - val_loss: 0.1489 - val_accuracy: 0.9510\n",
      "Epoch 7/100\n",
      "6125/6125 [==============================] - 38s 6ms/step - loss: 0.1375 - accuracy: 0.9531 - val_loss: 0.1426 - val_accuracy: 0.9517\n",
      "Epoch 8/100\n",
      "6125/6125 [==============================] - 38s 6ms/step - loss: 0.1336 - accuracy: 0.9545 - val_loss: 0.1418 - val_accuracy: 0.9520\n",
      "Epoch 9/100\n",
      "6125/6125 [==============================] - 39s 6ms/step - loss: 0.1302 - accuracy: 0.9559 - val_loss: 0.1515 - val_accuracy: 0.9505\n",
      "Epoch 10/100\n",
      "6125/6125 [==============================] - 40s 7ms/step - loss: 0.1271 - accuracy: 0.9566 - val_loss: 0.1430 - val_accuracy: 0.9525\n",
      "Epoch 11/100\n",
      "6125/6125 [==============================] - 40s 7ms/step - loss: 0.1242 - accuracy: 0.9575 - val_loss: 0.1535 - val_accuracy: 0.9482\n",
      "Epoch 12/100\n",
      "6125/6125 [==============================] - 41s 7ms/step - loss: 0.1224 - accuracy: 0.9582 - val_loss: 0.1528 - val_accuracy: 0.9475\n",
      "Epoch 13/100\n",
      "6125/6125 [==============================] - 40s 6ms/step - loss: 0.1207 - accuracy: 0.9591 - val_loss: 0.1523 - val_accuracy: 0.9498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6a6b514940>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model_cnn.fit(X_train_pad_cnn, y_train, \n",
    "          epochs=100, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 95.148%\n"
     ]
    }
   ],
   "source": [
    "res = model_cnn.evaluate(X_test_pad_cnn, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Good_test = \"Great room and the view was amazing. The buffet breakfast had so many options and so much of everything as well. \\\n",
    "    The shops under the hotel were really good and also lots of little eating places too.\\\n",
    "        Raffles is a few mins walk and you can walk to Clarke Quay as well.\\\n",
    "            Perfect location and would highly recommend.\"\n",
    "            \n",
    "Bad_test = \"the view was horrible. breakfast had so little options and so little of everything. \\\n",
    "    The shops under the hotel were really terrible and very little eating places too.\"\n",
    "    \n",
    "Good_test = preprocessing_cnn(Good_test)\n",
    "Good_test_token = tk.texts_to_sequences(Good_test)\n",
    "Good_test_pad = pad_sequences(Good_test_token, dtype='float32', padding='post',maxlen=max_length_cnn)\n",
    "Bad_test = preprocessing_cnn(Bad_test)\n",
    "Bad_test_token = tk.texts_to_sequences(Bad_test)\n",
    "Bad_test_pad = pad_sequences(Bad_test_token, dtype='float32', padding='post',maxlen=max_length_cnn)\n",
    "\n",
    "# Predict the sentiment of the good sentence\n",
    "predictions_good = model_cnn.predict(Good_test_pad)\n",
    "predicted_label_good = int(predictions_good[0][0] > 0.5)\n",
    "\n",
    "# Predict the sentiment of the bad sentence\n",
    "predictions_bad = model_cnn.predict(Bad_test_pad)\n",
    "predicted_label_bad = int(predictions_bad[0][0] > 0.5)\n",
    "\n",
    "predicted_label_good,predicted_label_bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36721548],\n",
       "       [0.44678435],\n",
       "       [0.4112868 ],\n",
       "       [0.32420632],\n",
       "       [0.23397201],\n",
       "       [0.4047879 ],\n",
       "       [0.44678435],\n",
       "       [0.71116257],\n",
       "       [0.71116257],\n",
       "       [0.34315896],\n",
       "       [0.4047879 ],\n",
       "       [0.32420632],\n",
       "       [0.02103406],\n",
       "       [0.43336147],\n",
       "       [0.4047879 ],\n",
       "       [0.23397201],\n",
       "       [0.34924808],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.6097729 ],\n",
       "       [0.5382514 ],\n",
       "       [0.4112868 ],\n",
       "       [0.42350602],\n",
       "       [0.4047879 ],\n",
       "       [0.30724353],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.32420632],\n",
       "       [0.34315896],\n",
       "       [0.32420632],\n",
       "       [0.416127  ],\n",
       "       [0.41128668],\n",
       "       [0.4047879 ],\n",
       "       [0.23397201],\n",
       "       [0.34924808],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.30724353],\n",
       "       [0.49489826],\n",
       "       [0.39102545],\n",
       "       [0.39102545],\n",
       "       [0.4112868 ],\n",
       "       [0.23397201],\n",
       "       [0.4047879 ],\n",
       "       [0.30724353],\n",
       "       [0.44678435],\n",
       "       [0.4112868 ],\n",
       "       [0.32420632],\n",
       "       [0.42214417],\n",
       "       [0.39102545],\n",
       "       [0.32420632],\n",
       "       [0.344249  ],\n",
       "       [0.23397201],\n",
       "       [0.4047879 ],\n",
       "       [0.34924808],\n",
       "       [0.32420632],\n",
       "       [0.6097729 ],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.344249  ],\n",
       "       [0.71116257],\n",
       "       [0.4047879 ],\n",
       "       [0.34315887],\n",
       "       [0.3242063 ],\n",
       "       [0.02103406],\n",
       "       [0.44960028],\n",
       "       [0.4047879 ],\n",
       "       [0.71116257],\n",
       "       [0.36559427],\n",
       "       [0.23397201],\n",
       "       [0.5382514 ],\n",
       "       [0.71116257],\n",
       "       [0.02103406],\n",
       "       [0.4047879 ],\n",
       "       [0.32420632],\n",
       "       [0.02103406],\n",
       "       [0.43336147],\n",
       "       [0.4047879 ],\n",
       "       [0.344249  ],\n",
       "       [0.71116257],\n",
       "       [0.4047879 ],\n",
       "       [0.34315896],\n",
       "       [0.49489826],\n",
       "       [0.25979057],\n",
       "       [0.34924808],\n",
       "       [0.4047879 ],\n",
       "       [0.71116257],\n",
       "       [0.39102545],\n",
       "       [0.4047879 ],\n",
       "       [0.4112868 ],\n",
       "       [0.6097729 ],\n",
       "       [0.4112868 ],\n",
       "       [0.44678435],\n",
       "       [0.44960028],\n",
       "       [0.23397206],\n",
       "       [0.34924793],\n",
       "       [0.5382514 ],\n",
       "       [0.02103406],\n",
       "       [0.36721548],\n",
       "       [0.4047879 ],\n",
       "       [0.32420632],\n",
       "       [0.4047879 ],\n",
       "       [0.42350602],\n",
       "       [0.4112868 ],\n",
       "       [0.60824025],\n",
       "       [0.60824025],\n",
       "       [0.4047879 ],\n",
       "       [0.23397201],\n",
       "       [0.34924808],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.344249  ],\n",
       "       [0.34924808],\n",
       "       [0.71116257],\n",
       "       [0.36559427],\n",
       "       [0.4047879 ],\n",
       "       [0.49489826],\n",
       "       [0.02103406],\n",
       "       [0.43336147],\n",
       "       [0.4112868 ],\n",
       "       [0.44678435],\n",
       "       [0.4047879 ],\n",
       "       [0.23397201],\n",
       "       [0.34924808],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.34924793],\n",
       "       [0.7111625 ],\n",
       "       [0.23397201],\n",
       "       [0.4112868 ],\n",
       "       [0.60824025],\n",
       "       [0.4047879 ],\n",
       "       [0.30724353],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.44678435],\n",
       "       [0.4112868 ],\n",
       "       [0.32420632],\n",
       "       [0.60824025],\n",
       "       [0.60824025],\n",
       "       [0.44960028],\n",
       "       [0.4047879 ],\n",
       "       [0.36721548],\n",
       "       [0.71116257],\n",
       "       [0.71116257],\n",
       "       [0.43336147],\n",
       "       [0.4047879 ],\n",
       "       [0.32420632],\n",
       "       [0.02103406],\n",
       "       [0.43336147],\n",
       "       [0.4047879 ],\n",
       "       [0.32420632],\n",
       "       [0.60824025],\n",
       "       [0.344249  ],\n",
       "       [0.71116257],\n",
       "       [0.4047879 ],\n",
       "       [0.60824025],\n",
       "       [0.71116257],\n",
       "       [0.23397206],\n",
       "       [0.4047879 ],\n",
       "       [0.71116257],\n",
       "       [0.39102545],\n",
       "       [0.4047879 ],\n",
       "       [0.60824025],\n",
       "       [0.5382514 ],\n",
       "       [0.23397201],\n",
       "       [0.23397201],\n",
       "       [0.60824025],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.4112868 ],\n",
       "       [0.32420632],\n",
       "       [0.23397201],\n",
       "       [0.4047879 ],\n",
       "       [0.36559427],\n",
       "       [0.60824025],\n",
       "       [0.32420632],\n",
       "       [0.25979057],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.23397201],\n",
       "       [0.71116257],\n",
       "       [0.71116257],\n",
       "       [0.4047879 ],\n",
       "       [0.44678435],\n",
       "       [0.32420632],\n",
       "       [0.39102545],\n",
       "       [0.39102545],\n",
       "       [0.60824025],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.30724347],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.32420632],\n",
       "       [0.4047879 ],\n",
       "       [0.39102545],\n",
       "       [0.4112868 ],\n",
       "       [0.42350602],\n",
       "       [0.4047879 ],\n",
       "       [0.34315896],\n",
       "       [0.5382514 ],\n",
       "       [0.02103406],\n",
       "       [0.4047879 ],\n",
       "       [0.42350602],\n",
       "       [0.32420632],\n",
       "       [0.60824025],\n",
       "       [0.42214417],\n",
       "       [0.4047879 ],\n",
       "       [0.32420632],\n",
       "       [0.02103406],\n",
       "       [0.43336147],\n",
       "       [0.4047879 ],\n",
       "       [0.44960028],\n",
       "       [0.71116257],\n",
       "       [0.49489826],\n",
       "       [0.4047879 ],\n",
       "       [0.25979057],\n",
       "       [0.32420632],\n",
       "       [0.02103406],\n",
       "       [0.4047879 ],\n",
       "       [0.42350602],\n",
       "       [0.3242063 ],\n",
       "       [0.6082401 ],\n",
       "       [0.42214417],\n",
       "       [0.4047879 ],\n",
       "       [0.23397201],\n",
       "       [0.71116257],\n",
       "       [0.4047879 ],\n",
       "       [0.25979057],\n",
       "       [0.60824025],\n",
       "       [0.32420632],\n",
       "       [0.44678435],\n",
       "       [0.42214417],\n",
       "       [0.4112868 ],\n",
       "       [0.4047879 ],\n",
       "       [0.38788736],\n",
       "       [0.49489826],\n",
       "       [0.32420632],\n",
       "       [0.44960028],\n",
       "       [0.4047879 ],\n",
       "       [0.32420632],\n",
       "       [0.4047879 ],\n",
       "       [0.42350602],\n",
       "       [0.4112868 ],\n",
       "       [0.60824025],\n",
       "       [0.60824025],\n",
       "       [0.4047879 ],\n",
       "       [0.36559427],\n",
       "       [0.4112868 ],\n",
       "       [0.44678435],\n",
       "       [0.39102545],\n",
       "       [0.4112868 ],\n",
       "       [0.25979057],\n",
       "       [0.23397206],\n",
       "       [0.4047879 ],\n",
       "       [0.60824025],\n",
       "       [0.71116257],\n",
       "       [0.25979057],\n",
       "       [0.32420632],\n",
       "       [0.23397201],\n",
       "       [0.5382514 ],\n",
       "       [0.71116257],\n",
       "       [0.02103406],\n",
       "       [0.4047879 ],\n",
       "       [0.32420632],\n",
       "       [0.02103406],\n",
       "       [0.43336147],\n",
       "       [0.4047879 ],\n",
       "       [0.42350602],\n",
       "       [0.71116257],\n",
       "       [0.49489826],\n",
       "       [0.60824025],\n",
       "       [0.43336147],\n",
       "       [0.4047879 ],\n",
       "       [0.34924808],\n",
       "       [0.5382514 ],\n",
       "       [0.36721548],\n",
       "       [0.34924808],\n",
       "       [0.60824025],\n",
       "       [0.44960028],\n",
       "       [0.4047879 ],\n",
       "       [0.44678435],\n",
       "       [0.4112868 ],\n",
       "       [0.25979057],\n",
       "       [0.71116257],\n",
       "       [0.34315887],\n",
       "       [0.34315887],\n",
       "       [0.41128668],\n",
       "       [0.02103407],\n",
       "       [0.43336132]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying 1D-CNN (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train_cnn2 = max(len(list) for list in X_train)\n",
    "max_test_cnn2 = max(len(list) for list in X_test)\n",
    "\n",
    "max_length_cnn2 =max(max_train_cnn2,max_test_cnn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the training and test sentences\n",
    "X_train_embed_cnn2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_cnn2 = embedding(word2vec_transfer, X_test)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_cnn2 = pad_sequences(X_train_embed_cnn2, dtype='float32', padding='post', maxlen=max_length_cnn2)\n",
    "X_test_pad_cnn2 = pad_sequences(X_test_embed_cnn2, dtype='float32', padding='post', maxlen=max_length_cnn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1D\n",
    "model_cnn2 = Sequential()\n",
    "model_cnn2.add(layers.Conv1D(filters=20, kernel_size=3))\n",
    "model_cnn2.add(layers.Flatten())\n",
    "model_cnn2.add(layers.Dense(10, activation=\"relu\"))\n",
    "model_cnn2.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model_cnn2.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 1s 37ms/step - loss: 0.7516 - accuracy: 0.5631 - val_loss: 0.6468 - val_accuracy: 0.5556\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.5998 - accuracy: 0.7020 - val_loss: 0.6656 - val_accuracy: 0.4971\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 21ms/step - loss: 0.4637 - accuracy: 0.7753 - val_loss: 0.6689 - val_accuracy: 0.5322\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.3283 - accuracy: 0.8712 - val_loss: 0.6994 - val_accuracy: 0.5731\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.2327 - accuracy: 0.9419 - val_loss: 0.7568 - val_accuracy: 0.5965\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 19ms/step - loss: 0.1771 - accuracy: 0.9470 - val_loss: 0.9530 - val_accuracy: 0.5380\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f20ac8b93c0>"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model_cnn2.fit(X_train_pad_cnn2, y_train, \n",
    "          epochs=100, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy evaluated on the test set is of 58.848%\n"
     ]
    }
   ],
   "source": [
    "res = model_cnn2.evaluate(X_test_pad_cnn2, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step\n",
      "5/5 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Good_test = \"Great room and the view was amazing. The buffet breakfast had so many options and so much of everything as well. \\\n",
    "    The shops under the hotel were really good and also lots of little eating places too.\\\n",
    "        Raffles is a few mins walk and you can walk to Clarke Quay as well.\\\n",
    "            Perfect location and would highly recommend.\"\n",
    "            \n",
    "Bad_test = \"the view was horrible. breakfast had so little options and so little of everything. \\\n",
    "    The shops under the hotel were really terrible and very little eating places too.\"\n",
    "    \n",
    "Good_test = preprocessing_cnn(Good_test)\n",
    "Good_test = embedding(word2vec_transfer, Good_test)\n",
    "Good_test_pad = pad_sequences(Good_test, dtype='float32', padding='post',maxlen=max_length_cnn2)\n",
    "Bad_test = preprocessing_cnn(Bad_test)\n",
    "Bad_test = embedding(word2vec_transfer, Bad_test)\n",
    "Bad_test_pad = pad_sequences(Bad_test, dtype='float32', padding='post',maxlen=max_length_cnn2)\n",
    "\n",
    "# Predict the sentiment of the good sentence\n",
    "predictions_good = model_cnn2.predict(Good_test_pad)\n",
    "predicted_label_good = int(predictions_good[0][0] > 0.5)\n",
    "\n",
    "# Predict the sentiment of the bad sentence\n",
    "predictions_bad = model_cnn2.predict(Bad_test_pad)\n",
    "predicted_label_bad = int(predictions_bad[0][0] > 0.5)\n",
    "\n",
    "predicted_label_good,predicted_label_bad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords:\n",
      "room\n",
      "staff\n",
      "location\n",
      "hotel\n",
      "breakfast\n",
      "good\n",
      "great\n",
      "bed\n",
      "friendly\n",
      "clean\n",
      "helpful\n",
      "stay\n",
      "small\n",
      "nice\n",
      "comfortable\n",
      "excellent\n",
      "would\n",
      "service\n",
      "get\n",
      "bathroom\n",
      "nothing\n",
      "like\n",
      "really\n",
      "could\n",
      "bar\n",
      "lovely\n",
      "night\n",
      "u\n",
      "one\n",
      "positive\n",
      "walk\n",
      "close\n",
      "restaurant\n",
      "check\n",
      "shower\n",
      "make\n",
      "time\n",
      "everything\n",
      "book\n",
      "station\n",
      "view\n",
      "london\n",
      "need\n",
      "go\n",
      "also\n",
      "well\n",
      "reception\n",
      "work\n",
      "even\n",
      "price\n",
      "little\n",
      "bite\n",
      "area\n",
      "day\n",
      "food\n",
      "perfect\n",
      "facility\n",
      "quiet\n",
      "comfy\n",
      "floor\n",
      "place\n",
      "size\n",
      "amaze\n",
      "pay\n",
      "park\n",
      "ask\n",
      "use\n",
      "free\n",
      "window\n",
      "give\n",
      "coffee\n",
      "tube\n",
      "take\n",
      "next\n",
      "door\n",
      "water\n",
      "much\n",
      "back\n",
      "love\n",
      "quite\n",
      "look\n",
      "wifi\n",
      "minute\n",
      "poor\n",
      "find\n",
      "air\n",
      "hot\n",
      "expensive\n",
      "come\n",
      "welcome\n",
      "upgrade\n",
      "fantastic\n",
      "extremely\n",
      "lot\n",
      "money\n",
      "modern\n",
      "leave\n",
      "noisy\n",
      "better\n",
      "around\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from heapq import nlargest\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Convert set to list, remove 'again', and convert back to set\n",
    "word_list = list(stop_words)\n",
    "word_list.append('negative')\n",
    "stop_words = set(word_list)\n",
    "\n",
    "# Function to preprocess sentences\n",
    "def preprocess(sentences):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_frequencies = defaultdict(int)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word.isalnum()]\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "    return word_frequencies\n",
    "\n",
    "# Function to extract keywords using TextRank\n",
    "def extract_keywords(sentences, num_keywords=100):\n",
    "    word_frequencies = preprocess(sentences)\n",
    "\n",
    "    # Calculate weighted word frequencies\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "\n",
    "    # Get top N keywords with highest scores\n",
    "    top_keywords = nlargest(num_keywords, word_frequencies, key=word_frequencies.get)\n",
    "    return top_keywords\n",
    "\n",
    "# Extract keywords from the list of sentences\n",
    "keywords = extract_keywords(X_train)\n",
    "\n",
    "# Print the extracted keywords\n",
    "print(\"Extracted Keywords:\")\n",
    "for keyword in keywords:\n",
    "    print(keyword)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
