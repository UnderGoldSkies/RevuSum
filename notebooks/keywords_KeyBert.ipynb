{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. read the train data and call basic cleaning function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data from a csv file and store in a dataframe with only 4 columns we need\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/Hotel_Reviews.csv')\n",
    "\n",
    "review_df = df[['Hotel_Name', 'Review_Date', 'Positive_Review', 'Negative_Review']]\n",
    "review_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String to make the review text invalid\n",
    "# Positive_Review: invalid reviews are with the following strings:\n",
    "test_pos_invalid_content = 'there are no comments available for this review|everything'\n",
    "# Negative_Review: invalid reviews are with the following strings:\n",
    "test_neg_invalid_content = 'nothing|n/a|none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Positive_Review: invalid reviews are with the following strings:'There are no comments available for this review', 'everything'\n",
    "train_pos_invalid_content = 'nothing|everything|no positive'\n",
    "# Negative_Review: invalid reviews are with the following strings:\n",
    "train_neg_invalid_content = 'nothing|everything|anything|no negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Add the directory containing the module to the Python path\n",
    "sys.path.append('/Users/zengsheng/code/TechLah/RevuSum')\n",
    "\n",
    "# Import the module that contains the function\n",
    "import app.basic_review_clean as brc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reviews for one hotel\n",
    "review_df_now = review_df[review_df['Hotel_Name'] == 'Hotel Arena'] \n",
    "review_df_now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test the function ###\n",
    "cleaned_review_df = brc.clean_hotel_reviews(review_df_now, train_pos_invalid_content, train_neg_invalid_content)\n",
    "print(cleaned_review_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_review_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 read the test data and call basic cleaning function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# this is cleaned data, so no need to basic clean again\n",
    "test_data_path = '/Users/zengsheng/code/TechLah/RevuSum/data/cleaned_test_data_5.pkl' # test data pkl file path\n",
    "# Read the DataFrame from the pickle file\n",
    "review_df = pd.read_pickle(test_data_path)\n",
    "review_df['Hotel_Name'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the reviews for one hotel\n",
    "hotel_name = 'Ibis Budget Singapore Pearl'\n",
    "cleaned_review_df = review_df[review_df['Hotel_Name'] == hotel_name] \n",
    "cleaned_review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    Holiday Inn Express Singapore Katong, an IHG H...\n",
       "12                          Ibis Budget Singapore Pearl\n",
       "12                         ibis Styles Singapore Albert\n",
       "12    Village Hotel Albert Court by Far East Hospita...\n",
       "Name: Hotel_Name, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_review_df['Positive_Review']\n",
    "review_df['Hotel_Name'][12]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "def preprocessing (text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, ' ') # Remove Punctuation\n",
    "    lowercased = text.lower() # Lower Case\n",
    "    tokenized = word_tokenize(lowercased) # Tokenize\n",
    "  \n",
    "    # tag each word with its part of speech\n",
    "    tagged_words = pos_tag(tokenized)\n",
    "    # remove adj and adv\n",
    "    filtered_words = [word for word, tag in tagged_words if tag not in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']]\n",
    "\n",
    "    words_only = [word for word in filtered_words if word.isalpha()] # Remove numbers\n",
    "    stop_words = set(stopwords.words('english')) # Make stopword lists \n",
    "    #add more stop words\n",
    "    stop_words.update(['hotel', 'booking', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'])\n",
    "    #print(len(stop_words))\n",
    "    \n",
    "    without_stopwords = [word for word in words_only if not word in stop_words] # Remove Stop Words\n",
    "    lemma=WordNetLemmatizer() # Initiate Lemmatizer\n",
    "    lemmatized = [lemma.lemmatize(word) for word in without_stopwords] # Lemmatize\n",
    "    cleaned = ' '.join(lemmatized) # Join back to a string\n",
    "    # print(f'{lowercased=}\\n', f'{tokenized=}\\n', f'{filtered_words=}\\n', f'{without_stopwords=}\\n', f'{words_only=}\\n', f'{lemmatized=}\\n', f'{cleaned=}\\n')\n",
    "    # print()\n",
    "    return cleaned\n",
    "\n",
    "preprocessing(cleaned_review_df['Positive_Review'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to preprocess the positive and negative reviews and merge them into one column\n",
    "# drop those rows with empty processed_text\n",
    "def preprocess_all(cleaned_review_df):\n",
    "\n",
    "    # preporcessing the positive and negative reviews and save them in two new columns:\n",
    "    cleaned_review_df['pos_processed_text'] = cleaned_review_df.Positive_Review.apply(lambda x: preprocessing(x) if not pd.isna(x) else x)\n",
    "    cleaned_review_df['neg_processed_text'] = cleaned_review_df.Negative_Review.apply(lambda x: preprocessing(x) if not pd.isna(x) else x)\n",
    "    df_now = cleaned_review_df.copy()\n",
    "\n",
    "    # merge the positive and negative reviews into one new column:'processed_text'\n",
    "    cleaned_review_df['processed_text'] = cleaned_review_df['pos_processed_text'] + cleaned_review_df['neg_processed_text']\n",
    "\n",
    "\n",
    "\n",
    "    #drop those rows with empty processed_text\n",
    "    df_now = cleaned_review_df[['processed_text']].dropna()\n",
    "    return df_now\n",
    "\n",
    "doc_df = preprocess_all(cleaned_review_df)\n",
    "doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dimension of the doc list\n",
    "type(doc_df['processed_text'][1])\n",
    "#convert df seriral to a list\n",
    "doc_list = doc_df['processed_text'].tolist()\n",
    "doc_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Keyword Extraction using KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# function to get the keywords from the text using KeyBERT\n",
    "def keywords_extract(text, top_n=5):\n",
    "    sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    kw_model = KeyBERT(model=sentence_model)\n",
    "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), top_n=top_n)\n",
    "    return keywords\n",
    "\n",
    "keywords = keywords_extract(doc_list, 100)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room', 'location', 'staff', 'bus', 'bed']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten the list of list\n",
    "one_list = [item for sublist in keywords for item in sublist]\n",
    "one_list\n",
    "\n",
    "# convert the list to a DataFrame\n",
    "keyword_df = pd.DataFrame(one_list, columns=['keyword', 'score'])\n",
    "keyword_df\n",
    "# get the top 10 keywords by groupby keyword and get the sum of the score\n",
    "top_keyword = keyword_df.groupby('keyword').sum().sort_values(by='score', ascending=False).head(5)\n",
    "\n",
    "#get the the keywords list without the number\n",
    "top_keyword.index.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 10 keywords by groupby keyword and get the sum of the score\n",
    "keyword_df.groupby('keyword').sum().sort_values(by='score', ascending=False).head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### may not be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def remove_adj_adv(sentence):\n",
    "    \n",
    "\n",
    "    #load small english model from spacy: https://spacy.io/models/en\n",
    "    nlp = spacy.load('en_core_web_sm') #you can use other methods\n",
    "    # excluded tags\n",
    "    excluded_tags = {\"ADJ\", \"ADV\"} #\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"ADP\", \"PROPN\"\n",
    "\n",
    "    new_sentence = []\n",
    "    for token in nlp(sentence):\n",
    "        if token.pos_ not in excluded_tags:\n",
    "            new_sentence.append(token.text)\n",
    "    new_sentence_str = \" \".join(new_sentence)\n",
    "    return new_sentence_str\n",
    "\n",
    "#print and compare the original and the new sentence\n",
    "print(\"Before:\", test_df['Reviews_clean'][0])\n",
    "print(\"After:\", remove_adj_adv(test_df['Reviews_clean'][0]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load small english model from spacy: https://spacy.io/models/en\n",
    "nlp = spacy.load('en_core_web_sm') #you can use other methods\n",
    "nlp(test_df['Reviews_clean'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
